This file contains a collection of resources on ***GANs***.<br>
# Contents
* [Survey](#survey) 
* [GAN zoos](#zoos)
* [Video Generation](#vg)

## <span id="survey">Survey</span><br>
2021, IEEE, Karthika. S [Generative Adversarial Network (GAN): a general review on different variants of GAN and applications](https://ieeexplore.ieee.org/abstract/document/9489160)<br>
2020.1.20, IEEE, Jie Gui [A review on generative adversarial networks: Algorithms, theory, and applications](https://arxiv.org/pdf/2001.06937)<br>

## <span id="zoos">GAN Zoos</span><br>
2014.1.10, arXiv, Ian J. Goodfellow [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf)<br> 
2014, arXiv, Mehdi Mirza [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784.pdf%EF%BC%88CGAN%EF%BC%89)<br>
2016, NeurIPS, Xi Chen [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf)<br>
2017, ICCV, Jun-Yan Zhu [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)<br>
2016, NeurIPS, Sebastian Nowozin [f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization](https://proceedings.neurips.cc/paper/2016/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf)<br>

## <span id="vg">Video Generation</span><br>
2016, ICLR, Michael Mathieu [Deep multi-scale video prediction beyond mean square error](https://arxiv.org/pdf/1511.05440.pdf%5D)<br>
2016, NeurIPS, Carl Vondrick [Generating videos with scene dynamics](https://proceedings.neurips.cc/paper/2016/file/04025959b191f8f9de3f924f0940515f-Paper.pdf)<br>
2016, arXiv, E Santana [Learning a driving simulator](https://arxiv.org/pdf/1608.01230.pdf?iframe=true&width=1480&height=620)<br>
2017, arXiv, Ruben Villegas [Decomposing motion and content for natural video sequence prediction](https://arxiv.org/pdf/1706.08033.pdf)<br>
2017, ICCV, Xiaodan Liang [Dual motion gan for future-flow embedded video prediction](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liang_Dual_Motion_GAN_ICCV_2017_paper.pdf)<br>
2017, ICCV, Jacob Walker [The pose knows: Video forecasting by generating pose futures](https://openaccess.thecvf.com/content_ICCV_2017/papers/Walker_The_Pose_Knows_ICCV_2017_paper.pdf)<br>
2018, arXiv, Ting-Chun Wang [Video-to-video synthesis](https://arxiv.org/pdf/1808.06601.pdf)<br>
2018, CVPR, Sergey Tulyakov [MoCoGAN: Decomposing Motion and Content for Video Generation](https://openaccess.thecvf.com/content_cvpr_2018/papers/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.pdf)<br>
2019, ICCV, [Everybody dance now](https://openaccess.thecvf.com/content_ICCV_2019/papers/Chan_Everybody_Dance_Now_ICCV_2019_paper.pdf)<br>
